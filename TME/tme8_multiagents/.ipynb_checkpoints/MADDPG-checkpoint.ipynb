{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3KQYu5QqbDmT",
    "outputId": "c65d8812-e708-4777-b900-10e2c8ba1a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m  ERROR: HTTP error 403 while getting http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
      "\u001b[31m  ERROR: Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
      "\u001b[31mERROR: Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl for URL http://download.pytorch.org/whl/cu101/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\u001b[0m\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.10)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.17.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
      "Collecting ludwig\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/59/59a658a5846cda34fc3832992e28e65d727c54a576ff48bcb3c90c3072e8/ludwig-0.2.1.tar.gz (164kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 3.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: Cython>=0.25 in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.29.15)\n",
      "Requirement already satisfied: h5py>=2.6 in /usr/local/lib/python3.6/dist-packages (from ludwig) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from ludwig) (1.17.5)\n",
      "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.25.3)\n",
      "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.6/dist-packages (from ludwig) (1.4.1)\n",
      "Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.8.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.22.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from ludwig) (4.28.1)\n",
      "Collecting tensorflow==1.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
      "\u001b[K     |████████████████████████████████| 109.2MB 31kB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from ludwig) (3.13)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from ludwig) (0.9.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.6->ludwig) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->ludwig) (2.6.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->ludwig) (2018.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ludwig) (0.14.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (3.10.0)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K     |████████████████████████████████| 491kB 10.9MB/s \n",
      "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 10.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (1.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (0.1.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (0.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (0.34.2)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0->ludwig) (1.27.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0->ludwig) (45.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->ludwig) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0->ludwig) (3.2.1)\n",
      "Building wheels for collected packages: ludwig\n",
      "  Building wheel for ludwig (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ludwig: filename=ludwig-0.2.1-cp36-none-any.whl size=226574 sha256=d941a8149dff7915f460ca4775017e4ce00164d5f15064c031194c6663f7d8b7\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/5d/fd/3796fa772d73a3c0888e78346f28f54100e5d5ef5562890426\n",
      "Successfully built ludwig\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow, ludwig\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "  Found existing installation: tensorflow 1.15.0\n",
      "    Uninstalling tensorflow-1.15.0:\n",
      "      Successfully uninstalled tensorflow-1.15.0\n",
      "Successfully installed ludwig-0.2.1 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
     ]
    }
   ],
   "source": [
    "googlecolab = True\n",
    "\n",
    "if googlecolab:\n",
    "    from os.path import exists\n",
    "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "    !pip install gym\n",
    "    !pip install ludwig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CRfGZpBec09H"
   },
   "source": [
    "#### Importation du gridworld à partir du local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "IICVEPn7gaaO",
    "outputId": "6add205b-94b4-4e60-e1cf-8ecd441cb17c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "716arOmZjT9T",
    "outputId": "09a86478-2567-4e38-f3f8-9d0ff86105ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gridworld_env.py  gridworldPlans  __init__.py  __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls drive/'My Drive'/gridworld/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdKGE9dEl090"
   },
   "outputs": [],
   "source": [
    "cp -r drive/'My Drive'/gridworld/ ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S4-0QtgUmEEr"
   },
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "id": "CLhzK9tkb6r0",
    "outputId": "844cf8f9-cb63-40e4-fbcc-003510ceaa10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gridworld\n",
      "  Downloading https://files.pythonhosted.org/packages/80/92/a4c11b3a9e5db7bf331b4f6f2fbc1d4c4653368712fd8636fbaa6abc8158/gridworld-0.1.tar.gz\n",
      "Collecting arcade\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/c6/3cce024e33bb692fac87572aed5e127c5fc2a57ecc5235610f65ebb57dc7/arcade-2.3.5-py2.py3-none-any.whl (8.7MB)\n",
      "\u001b[K     |████████████████████████████████| 8.7MB 14.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gridworld) (1.17.5)\n",
      "Collecting pyglet-ffmpeg2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/7d/2d353a569480fc314fb9ddae2299bdb75cf86388ee8ffd4c9f7a3694460c/pyglet_ffmpeg2-0.1.17-py3-none-any.whl (62.2MB)\n",
      "\u001b[K     |████████████████████████████████| 62.2MB 46kB/s \n",
      "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from arcade->gridworld) (6.2.2)\n",
      "Collecting pytiled-parser\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/f9/0174b4774ce98d1761fa4b60dbea9aada59563e81cfd2c9f6de3132026e9/pytiled_parser-0.9.2-py3-none-any.whl\n",
      "Requirement already satisfied: pyglet in /usr/local/lib/python3.6/dist-packages (from arcade->gridworld) (1.4.10)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from pytiled-parser->arcade->gridworld) (19.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet->arcade->gridworld) (0.16.0)\n",
      "Building wheels for collected packages: gridworld\n",
      "  Building wheel for gridworld (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gridworld: filename=gridworld-0.1-cp36-none-any.whl size=4347 sha256=b1a3eb91924ee15a97d3f44fc247357ab1235abcf9508f807fbd63fbcfc8e2be\n",
      "  Stored in directory: /root/.cache/pip/wheels/61/50/33/8859b5be280b8b2b2f0dae3ff8495a025418c8adac718f46d4\n",
      "Successfully built gridworld\n",
      "Installing collected packages: pyglet-ffmpeg2, pytiled-parser, arcade, gridworld\n",
      "Successfully installed arcade-2.3.5 gridworld-0.1 pyglet-ffmpeg2-0.1.17 pytiled-parser-0.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "import gym\n",
    "!pip install gridworld\n",
    "import gridworld\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bJfXGljepWwp",
    "outputId": "59215d59-1577-4e60-f7ac-626a07b4474b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda=True\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7S1zws1menx"
   },
   "source": [
    "## Classe NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUR8U62_md_i"
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, inSize, outSize, layers=[]):\n",
    "        super(NN,self).__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for x in layers:\n",
    "            self.layers.append(nn.Linear(inSize,x))\n",
    "            inSize = x\n",
    "        self.layers.append(nn.Linear(inSize,outSize))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers[0](x)\n",
    "        for i in  range(1,len(self.layers)):\n",
    "            x = torch.nn.functional.prelu(x, torch.tensor(0.2).to(device))\n",
    "            x = self.layers[i](x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHxNMG279ibm"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "mVOnDVgD8K0c",
    "outputId": "95c34851-3dd0-4da4-c1e6-732544b0bef2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 64]           1,600\n",
      "            Linear-2               [-1, 1, 128]           8,320\n",
      "            Linear-3                 [-1, 1, 2]             258\n",
      "================================================================\n",
      "Total params: 10,178\n",
      "Trainable params: 10,178\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 0.04\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]           1,600\n",
      "            Linear-2                  [-1, 128]           8,576\n",
      "            Linear-3                   [-1, 64]           8,256\n",
      "            Linear-4                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 18,497\n",
      "Trainable params: 18,497\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 0.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "\n",
    "torch.manual_seed(999)\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor (Policy) Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        :state_dim (int): Dimension of each state\n",
    "        :action_dim (int): Dimension of each action\n",
    "        \"\"\"\n",
    "\n",
    "        super(ActorNetwork, self).__init__()\n",
    "    \n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters\n",
    "        \"\"\"\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Maps a state to actions\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic (State-Value) Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model\n",
    "        :state_dim (int): Dimension of each state\n",
    "        :action_dim (int): Dimension of each action\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.state_fc = nn.Linear(state_dim, 64)\n",
    "        self.fc1 = nn.Linear(action_dim+64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters\n",
    "        \"\"\"\n",
    "        self.state_fc.weight.data.uniform_(*hidden_init(self.state_fc))\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Maps a state-action pair to Q-values\n",
    "        \"\"\"\n",
    "        state, action = state.squeeze(), action.squeeze()\n",
    "        x = F.relu(self.state_fc(state))\n",
    "        x = torch.cat((x, action), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # summarize network structures using torchsummary\n",
    "    state_dim, action_dim = 24, 2\n",
    "    actor = ActorNetwork(state_dim, action_dim).to(device)\n",
    "    critic = CriticNetwork(state_dim, action_dim).to(device)\n",
    "    sum_res = summary(actor, (1, state_dim))\n",
    "    sum_res = summary(critic, [(1, state_dim), (1, action_dim)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BiQNB-UA9Y5-"
   },
   "source": [
    "# MADDP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlNOvzTD7u0S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    " \n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import copy\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "class MADDPG:\n",
    "    \"\"\"\n",
    "    The Multi-Agent consisting of two DDPG Agents\n",
    "    \"\"\"\n",
    "    def __init__(self,state_dim,\n",
    "                 action_dim,\n",
    "                 lr_actor,\n",
    "                 lr_critic,\n",
    "                 lr_decay,\n",
    "                 replay_buff_size,\n",
    "                 gamma,\n",
    "                 batch_size,\n",
    "                 random_seed, \n",
    "                 soft_update_tau):\n",
    "    \n",
    "        \n",
    "        super(MADDPG, self).__init__()\n",
    "        self.adversarial_agents = []\n",
    "        for i in range(len(state_dim)):\n",
    "          agent = DDPGAgent(state_dim[i],\n",
    "                 action_dim,\n",
    "                 )\n",
    "          self.adversarial_agents.append(agent)\n",
    "     # the agent self-plays with itself\n",
    "        \n",
    "    def get_actors(self):\n",
    "        \"\"\"\n",
    "        get actors of all the agents in the MADDPG object\n",
    "        \"\"\"\n",
    "        actors = [ddpg_agent.actor_local for ddpg_agent in self.adversarial_agents]\n",
    "        return actors\n",
    "\n",
    "    def get_target_actors(self):\n",
    "        \"\"\"\n",
    "        get target_actors of all the agents in the MADDPG object\n",
    "        \"\"\"\n",
    "        target_actors = [ddpg_agent.actor_target for ddpg_agent in self.adversarial_agents]\n",
    "        return target_actors\n",
    "\n",
    "    def act(self, states_all_agents, add_noise=False):\n",
    "        \"\"\"\n",
    "        get actions from all agents in the MADDPG object\n",
    "        \"\"\"\n",
    "        actions = [agent.act(state, add_noise) for agent, state in zip(self.adversarial_agents, states_all_agents)]\n",
    "        return np.stack(actions, axis=0)\n",
    "\n",
    "    def update(self, *experiences):\n",
    "        \"\"\"\n",
    "        update the critics and actors of all the agents\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        for agent_idx, agent in enumerate(self.adversarial_agents):\n",
    "            state = states[agent_idx]\n",
    "            action = actions[agent_idx]\n",
    "            reward = rewards[agent_idx]\n",
    "            next_state = next_states[agent_idx]\n",
    "            done = dones[agent_idx]\n",
    "            agent.update_model(state, action, reward, next_state, done)\n",
    "            \n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Save the model\n",
    "        \"\"\"\n",
    "        agent = self.adversarial_agents[0]\n",
    "        torch.save((agent.actor_local.state_dict(), agent.critic_local.state_dict()), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        \"\"\"\n",
    "        Load model and decay learning rate\n",
    "        \"\"\"\n",
    "        actor_state_dict, critic_state_dict = torch.load(path)\n",
    "        agent = self.adversarial_agents[0]\n",
    "        agent.actor_local.load_state_dict(actor_state_dict)\n",
    "        agent.actor_target.load_state_dict(actor_state_dict)\n",
    "        agent.critic_local.load_state_dict(critic_state_dict)\n",
    "        agent.critic_target.load_state_dict(critic_state_dict)\n",
    "        agent.lr_actor *= agent.lr_decay\n",
    "        agent.lr_critic *= agent.lr_decay\n",
    "        for group in agent.actor_optimizer.param_groups:\n",
    "            group['lr'] = agent.lr_actor\n",
    "        for group in agent.critic_optimizer.param_groups:\n",
    "            group['lr'] = agent.lr_critic\n",
    "        \n",
    "        for i in range(1, len(self.adversarial_agents)):\n",
    "            self.adversarial_agents[i] = agent\n",
    "            \n",
    "        print(\"Loaded models!\")\n",
    "            \n",
    "\n",
    "class DDPGAgent:\n",
    "    \"\"\"\n",
    "    A DDPG Agent\n",
    "    \"\"\"    \n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 lr_actor = 1e-4,\n",
    "                 lr_critic = 1e-4,\n",
    "                 lr_decay = .95,\n",
    "                 replay_buff_size = 10000,\n",
    "                 gamma = .9,\n",
    "                 batch_size = 128,\n",
    "                 random_seed = 42,\n",
    "                 soft_update_tau = 1e-3\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize model\n",
    "        \"\"\"\n",
    "        self.lr_actor = lr_actor\n",
    "        self.gamma = gamma\n",
    "        self.lr_critic = lr_critic\n",
    "        self.lr_decay = lr_decay\n",
    "        self.tau = soft_update_tau\n",
    "        self.actor_local = ActorNetwork(state_dim, action_dim).to(device=device)\n",
    "        self.actor_target = ActorNetwork(state_dim, action_dim).to(device=device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=self.lr_actor)\n",
    "        \n",
    "        self.critic_local = CriticNetwork(state_dim, action_dim).to(device=device)\n",
    "        self.critic_target = CriticNetwork(state_dim, action_dim).to(device=device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=self.lr_critic)\n",
    "        \n",
    "        self.noise = OUNoise(action_dim, random_seed)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_dim, replay_buff_size, batch_size, random_seed)\n",
    "  \n",
    "        \n",
    "    def update_model(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        \n",
    "        :experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "        :gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        if not self.memory.is_ready():\n",
    "            return\n",
    "        \n",
    "        experiences = self.memory.sample()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones)).detach()\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        y = Q_targets.mean().item()\n",
    "\n",
    "        critic_loss = F.smooth_l1_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        #--------------------------- PLOTING -----------------------------------#\n",
    "    \n",
    "        \n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, self.tau)\n",
    "        self.soft_update(self.actor_local, self.actor_target, self.tau)   \n",
    "        \n",
    "    def act(self, state, noise_t=0.0):\n",
    "        \"\"\"\n",
    "        Returns actions for given state as per current policy.\n",
    "        \"\"\"\n",
    "        if random.random()<0.1:\n",
    "          v=random.random()\n",
    "          return np.array([v,1-v])\n",
    "        if len(np.shape(state)) == 1:\n",
    "            state = state.reshape(1,-1)\n",
    "        state = torch.from_numpy(state).float().to(device=device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        action += self.noise.sample() * noise_t\n",
    "        return np.clip(action, -1, 1).squeeze()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        :local_model: PyTorch model (weights will be copied from)\n",
    "        :target_model: PyTorch model (weights will be copied to)\n",
    "        :tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"\n",
    "    Ornstein-Uhlenbeck process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"\n",
    "        Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the internal state (= noise) to mean (mu).\n",
    "        \"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Update internal state and return it as a noise sample.\n",
    "        \"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Fixed-size buffer to store experience tuples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"\n",
    "        Initialize a ReplayBuffer object.\n",
    "        \n",
    "        :buffer_size (int): maximum size of buffer\n",
    "        :batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a new experience to memory.\n",
    "        \"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of experiences from memory.\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def is_ready(self):\n",
    "        return len(self.memory) > self.batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of internal memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4h4_l1M8-6q"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "yfK3nljuDATK",
    "outputId": "6d9e0f4b-a473-4bd6-a6c9-f377c310dd19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.17.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.6.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (45.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def draw(scores, path=\"fig.png\", title=\"Performance\", xlabel=\"Episode #\", ylabel=\"Score\"):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.title(title)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "4jGxC4YPHMKA",
    "outputId": "ba436e26-21b1-413d-87c6-87538fdaa74f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-15 21:07:05--  http://dac.lip6.fr/master/wp-content/uploads/2019/12/multiagent.zip\n",
      "Resolving dac.lip6.fr (dac.lip6.fr)... 132.227.201.33\n",
      "Connecting to dac.lip6.fr (dac.lip6.fr)|132.227.201.33|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 51346 (50K) [application/zip]\n",
      "Saving to: ‘multiagent.zip’\n",
      "\n",
      "multiagent.zip      100%[===================>]  50.14K   186KB/s    in 0.3s    \n",
      "\n",
      "2020-02-15 21:07:06 (186 KB/s) - ‘multiagent.zip’ saved [51346/51346]\n",
      "\n",
      "Archive:  /content/multiagent.zip\n",
      "   creating: multiagent/\n",
      "  inflating: multiagent/multi_discrete.py  \n",
      "  inflating: multiagent/rendering.py  \n",
      "  inflating: multiagent/environment.py  \n",
      "  inflating: multiagent/reraise.py   \n",
      "  inflating: multiagent/scenario.py  \n",
      "  inflating: multiagent/__init__.py  \n",
      "  inflating: multiagent/policy.py    \n",
      "  inflating: multiagent/core.py      \n",
      "   creating: multiagent/__pycache__/\n",
      "  inflating: multiagent/__pycache__/rendering.cpython-37.pyc  \n",
      "  inflating: multiagent/__pycache__/multi_discrete.cpython-37.pyc  \n",
      "  inflating: multiagent/__pycache__/reraise.cpython-37.pyc  \n",
      "  inflating: multiagent/__pycache__/environment.cpython-37.pyc  \n",
      "  inflating: multiagent/__pycache__/scenario.cpython-37.pyc  \n",
      "  inflating: multiagent/__pycache__/core.cpython-37.pyc  \n",
      "  inflating: multiagent/__pycache__/__init__.cpython-37.pyc  \n",
      "   creating: multiagent/scenarios/\n",
      "  inflating: multiagent/scenarios/simple_speaker_listener.py  \n",
      "  inflating: multiagent/scenarios/simple_world_comm.py  \n",
      "  inflating: multiagent/scenarios/simple_reference.py  \n",
      "  inflating: multiagent/scenarios/simple_adversary.py  \n",
      "  inflating: multiagent/scenarios/simple_spread.py  \n",
      "  inflating: multiagent/scenarios/simple_crypto.py  \n",
      "  inflating: multiagent/scenarios/simple_push.py  \n",
      "  inflating: multiagent/scenarios/simple_tag.py  \n",
      "  inflating: multiagent/scenarios/__init__.py  \n",
      "  inflating: multiagent/scenarios/simple.py  \n",
      "   creating: multiagent/scenarios/__pycache__/\n",
      "  inflating: multiagent/scenarios/__pycache__/simple_adversary.cpython-37.pyc  \n",
      "  inflating: multiagent/scenarios/__pycache__/simple_tag.cpython-37.pyc  \n",
      "  inflating: multiagent/scenarios/__pycache__/simple_spread.cpython-37.pyc  \n",
      "  inflating: multiagent/scenarios/__pycache__/__init__.cpython-37.pyc  \n"
     ]
    }
   ],
   "source": [
    "!wget \"http://dac.lip6.fr/master/wp-content/uploads/2019/12/multiagent.zip\"\n",
    "!unzip /content/multiagent.zip\n",
    "!rm /content/multiagent.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "F3bCsqBYHsnC",
    "outputId": "948cea44-33b7-4763-bda6-3d92e32a583e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  gridworld  multiagent  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OuZaJrJ77tDa",
    "outputId": "71c6ba7d-6a37-4561-d468-939fe284862e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 3\n",
      "Episodic 1 Score: -5974.4032\n",
      "Episodic 2 Score: -5110.2217\n",
      "Episodic 3 Score: -1132.5765\n",
      "Episodic 4 Score: -4569.9641\n",
      "Episodic 5 Score: -8815.0356\n",
      "Episodic 6 Score: -10095.5605\n",
      "Episodic 7 Score: -8250.3529\n",
      "Episodic 8 Score: -4136.9498\n",
      "Episodic 9 Score: -4468.6437\n",
      "Episodic 10 Score: -6881.8587\n",
      "Episodic 11 Score: -7852.4505\n",
      "Episodic 12 Score: -7616.1794\n",
      "Episodic 13 Score: -9847.2477\n",
      "Episodic 14 Score: -8719.5274\n",
      "Episodic 15 Score: -10215.6273\n",
      "Episodic 16 Score: -6633.6539\n",
      "Episodic 17 Score: -7763.0295\n",
      "Episodic 18 Score: -2730.4291\n",
      "Episodic 19 Score: -5752.8390\n",
      "Episodic 20 Score: -1763.2225\n",
      "Episodic 21 Score: -5476.8554\n",
      "Episodic 22 Score: -7680.0130\n",
      "Episodic 23 Score: -5237.2468\n",
      "Episodic 24 Score: -3696.1125\n",
      "Episodic 25 Score: -2974.1834\n",
      "Episodic 26 Score: -2774.8137\n",
      "Episodic 27 Score: -2837.3390\n",
      "Episodic 28 Score: -2744.8193\n",
      "Episodic 29 Score: -2090.5613\n",
      "Episodic 30 Score: -2094.5118\n",
      "Episodic 31 Score: -2126.8882\n",
      "Episodic 32 Score: -2330.5170\n",
      "Episodic 33 Score: -2148.5539\n",
      "Episodic 34 Score: -2722.5962\n",
      "Episodic 35 Score: -2102.5581\n",
      "Episodic 36 Score: -2464.7153\n",
      "Episodic 37 Score: -2575.5292\n",
      "Episodic 38 Score: -2804.9732\n",
      "Episodic 39 Score: -2854.2871\n",
      "Episodic 40 Score: -2693.5458\n",
      "Episodic 41 Score: -2276.4460\n",
      "Episodic 42 Score: -2546.4647\n",
      "Episodic 43 Score: -2157.3345\n",
      "Episodic 44 Score: -1949.7440\n",
      "Episodic 45 Score: -2233.6996\n",
      "Episodic 46 Score: -2928.4918\n",
      "Episodic 47 Score: -2388.8284\n",
      "Episodic 48 Score: -2618.1702\n",
      "Episodic 49 Score: -2009.6662\n",
      "Episodic 50 Score: -1887.1333\n",
      "Episodic 51 Score: -1726.0336\n",
      "Episodic 52 Score: -1912.5366\n",
      "Episodic 53 Score: -1430.3521\n",
      "Episodic 54 Score: -2143.9032\n",
      "Episodic 55 Score: -1641.9841\n",
      "Episodic 56 Score: -992.4907\n",
      "Episodic 57 Score: -1067.3280\n",
      "Episodic 58 Score: -679.6512\n",
      "Episodic 59 Score: -969.5461\n",
      "Episodic 60 Score: -880.3406\n",
      "Episodic 61 Score: -1032.9186\n",
      "Episodic 62 Score: -1042.7862\n",
      "Episodic 63 Score: -1080.5682\n",
      "Episodic 64 Score: -767.9841\n",
      "Episodic 65 Score: -968.8773\n",
      "Episodic 66 Score: -750.6164\n",
      "Episodic 67 Score: -724.2136\n",
      "Episodic 68 Score: -1039.6632\n",
      "Episodic 69 Score: -690.8379\n",
      "Episodic 70 Score: -915.0058\n",
      "Episodic 71 Score: -979.3159\n",
      "Episodic 72 Score: -871.6647\n",
      "Episodic 73 Score: -1021.0344\n",
      "Episodic 74 Score: -1112.8605\n",
      "Episodic 75 Score: -767.1874\n",
      "Episodic 76 Score: -1083.4001\n",
      "Episodic 77 Score: -780.4379\n",
      "Episodic 78 Score: -879.3734\n",
      "Episodic 79 Score: -859.4892\n",
      "Episodic 80 Score: -1016.5515\n",
      "Episodic 81 Score: -699.1214\n",
      "Episodic 82 Score: -1060.1854\n",
      "Episodic 83 Score: -1083.9381\n",
      "Episodic 84 Score: -981.4588\n",
      "Episodic 85 Score: -823.7162\n",
      "Episodic 86 Score: -944.1640\n",
      "Episodic 87 Score: -981.9997\n",
      "Episodic 88 Score: -1146.4766\n",
      "Episodic 89 Score: -1059.5356\n",
      "Episodic 90 Score: -1008.8563\n",
      "Episodic 91 Score: -873.2228\n",
      "Episodic 92 Score: -1075.9906\n",
      "Episodic 93 Score: -1072.1043\n",
      "Episodic 94 Score: -1591.2710\n",
      "Episodic 95 Score: -999.3433\n",
      "Episodic 96 Score: -1641.7419\n",
      "Episodic 97 Score: -901.1400\n",
      "Episodic 98 Score: -904.6912\n",
      "Episodic 99 Score: -1328.1896\n",
      "Episodic 100 Score: -1209.4666\n",
      "100 Episodic Everage Score: -2669.7194\n",
      "Episodic 101 Score: -1376.4900\n",
      "100 Episodic Everage Score: -2623.7402\n",
      "Episodic 102 Score: -938.8219\n",
      "100 Episodic Everage Score: -2582.0263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-7c1274a0f89e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m           \u001b[0mnext_states\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# send the action to the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m           \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m           \u001b[0mnoise_t\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mnoise_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m           \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-3dd54d58ba32>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, *experiences)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-3dd54d58ba32>\u001b[0m in \u001b[0;36mupdate_model\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m#--------------------------- PLOTING -----------------------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#from agent import MADDPG\n",
    "#from utils import draw\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import gym\n",
    "import multiagent\n",
    "import multiagent.scenarios\n",
    "import multiagent.scenarios.simple_tag as simple_tag\n",
    "import multiagent.scenarios.simple_tag as simple_spread\n",
    "import multiagent.scenarios.simple_tag as simple_adversary\n",
    "from multiagent.environment import MultiAgentEnv\n",
    "import multiagent.scenarios as scenarios\n",
    "from gym import wrappers, logger\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "unity_environment_path = \"./Tennis_Linux/Tennis.x86_64\"\n",
    "best_model_path = \"./best_model.checkpoint\" \n",
    "rollout_length = 3\n",
    "\n",
    "def make_env(scenario_name, benchmark=False):\n",
    "    '''\n",
    "    Creates a MultiAgentEnv object as env. This can be used similar to a gym\n",
    "    environment by calling env.reset() and env.step().\n",
    "    Use env.render() to view the environment on the screen.\n",
    "\n",
    "    Input:\n",
    "        scenario_name   :   name of the scenario from ./scenarios/ to be Returns\n",
    "                            (without the .py extension)\n",
    "        benchmark       :   whether you want to produce benchmarking data\n",
    "                            (usually only done during evaluation)\n",
    "\n",
    "    Some useful env properties (see environment.py):\n",
    "        .observation_space  :   Returns the observation space for each agent\n",
    "        .action_space       :   Returns the action space for each agent\n",
    "        .n                  :   Returns the number of Agents\n",
    "    '''\n",
    "    from multiagent.environment import MultiAgentEnv\n",
    "    import multiagent.scenarios as scenarios\n",
    "\n",
    "    # load scenario from script\n",
    "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
    "    # create world\n",
    "    world = scenario.make_world()\n",
    "    # create multiagent environment\n",
    "    world.dim_c = 0\n",
    "    if benchmark:\n",
    "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)\n",
    "    else:\n",
    "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)\n",
    "    env.discrete_action_space = False\n",
    "    env.discrete_action_input = False\n",
    "    scenario.reset_world(world)\n",
    "    return env,scenario,world\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # prepare environment\n",
    "    env,scenario,world = make_env('simple_spread')\n",
    "\n",
    "    \n",
    "    num_agents = len(env.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "    \n",
    "\n",
    "\n",
    "    num_episodes = 300\n",
    "    \n",
    "    state_size = env.observation_space\n",
    "    action_size = 2\n",
    "\n",
    "    o=env.reset()\n",
    "    state_size=[len(i) for i in o]\n",
    "\n",
    "    agent = MADDPG(state_size, \n",
    "                   action_size,\n",
    "                   lr_actor = 1e-5,\n",
    "                   lr_critic = 1e-4,\n",
    "                   lr_decay = .995,\n",
    "                   replay_buff_size = int(1e6),\n",
    "                   gamma = .95,\n",
    "                   batch_size = 64,\n",
    "                   random_seed = 999,\n",
    "                   soft_update_tau = 1e-3\n",
    "                 )\n",
    "    total_rewards = []\n",
    "    avg_scores = []\n",
    "    max_avg_score = -1\n",
    "    max_score = -1\n",
    "    threshold_init = 20\n",
    "    noise_t = 1.0\n",
    "    noise_decay = .995\n",
    "    worsen_tolerance = threshold_init  # for early-stopping training if consistently worsen for # episodes\n",
    "    reward = 0\n",
    "    num_agents = 3\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        obs = env.reset()   # reset the environment        \n",
    "        scores = np.zeros(num_agents)                        # initialize score array\n",
    "        dones = [False]*num_agents\n",
    "        #while not np.any(dones):\n",
    "        for i in range(100):\n",
    "          if np.any(dones):\n",
    "            break\n",
    "          actions = agent.act(obs,noise_t)              # select an action\n",
    "          next_states , rewards, dones, _ = env.step(actions)         # send the action to the environment     \n",
    "          \n",
    "          agent.update(obs, actions, rewards, next_states, dones)\n",
    "          noise_t *= noise_decay\n",
    "          scores += rewards       \n",
    "          obs = next_states                         \n",
    "          # update scores \n",
    "        \n",
    "        episode_score = np.max(scores)\n",
    "        total_rewards.append(episode_score)\n",
    "        print(\"Episodic {} Score: {:.4f}\".format(i_episode, episode_score))\n",
    "        \n",
    "        if max_score <= episode_score:                     \n",
    "            max_score = episode_score\n",
    "            agent.save(best_model_path)                     # save best model so far\n",
    "        \n",
    "        if len(total_rewards) >= 100:                       # record avg score for the latest 100 steps\n",
    "            latest_avg_score = sum(total_rewards[(len(total_rewards)-100):]) / 100\n",
    "            print(\"100 Episodic Everage Score: {:.4f}\".format(latest_avg_score))\n",
    "            avg_scores.append(latest_avg_score)\n",
    "          \n",
    "            if max_avg_score <= latest_avg_score:           # record better results\n",
    "                worsen_tolerance = threshold_init           # re-count tolerance\n",
    "                max_avg_score = latest_avg_score\n",
    "            else:                                           \n",
    "                if max_avg_score > 0.5:                     \n",
    "                    worsen_tolerance -= 1                   # count worsening counts\n",
    "                    print(\"Loaded from last best model.\")\n",
    "                    agent.load(best_model_path)             # continue from last best-model\n",
    "                if worsen_tolerance <= 0:                   # earliy stop training\n",
    "                    print(\"Early Stop Training.\")\n",
    "                    break\n",
    "                    \n",
    "    draw(total_rewards,\"./training_score_plot.png\", \"Training Scores (Per Episode)\")\n",
    "    draw(avg_scores,\"./training_100avgscore_plot.png\", \"Training Scores (Average of Latest 100 Episodes)\", ylabel=\"Avg. Score\")\n",
    "    env.close()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de MADDPG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
